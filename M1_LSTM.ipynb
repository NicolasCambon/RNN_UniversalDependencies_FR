{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "M1_LSTM.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rc7DMjd1uPpB",
        "colab_type": "code",
        "outputId": "33b84b95-9971-49f1-aeff-4518a4be93df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "!git clone https://github.com/NicolasCambon/RNN_UniversalDependencies_FR.git #Téléchargment des données"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'RNN_UniversalDependencies_FR'...\n",
            "remote: Enumerating objects: 55, done.\u001b[K\n",
            "remote: Counting objects: 100% (55/55), done.\u001b[K\n",
            "remote: Compressing objects: 100% (47/47), done.\u001b[K\n",
            "remote: Total 55 (delta 8), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (55/55), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5NiFTb58tA58",
        "colab_type": "code",
        "outputId": "768a818b-965d-4e72-c7e5-584618725b0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "def distance_verbe(fic):\n",
        "  try:\n",
        "    fichier = open(fic,\"r\")\n",
        "  except IOError:\n",
        "    print(\"le fichier\",fic, \"n'existe pas\")\n",
        "    return None\n",
        "  \n",
        "  liste=[]\n",
        "  for ligne in fichier :  \n",
        "    ligne = ligne.strip('\\n\\r') \n",
        "    seq = [x for x in ligne.split()]  \n",
        "    liste.append(seq)  \n",
        "  fichier.close()\n",
        "  \n",
        "  sentence = []\n",
        "  part=[]\n",
        "  for l in range (len(liste)-1):\n",
        "    if len(liste[l])!= 0:\n",
        "      part.append(liste[l])\n",
        "    else:\n",
        "      sentence.append(part)\n",
        "      part=[]\n",
        "  num=0\n",
        "  deno=0\n",
        "  for i in range (len(sentence)):\n",
        "    lis=[]\n",
        "    for j in range(len(sentence[i])):\n",
        "      if len(sentence[i][j])>8 and sentence[i][j][7]== 'root':\n",
        "        chiffre = sentence[i][j][0]\n",
        "        for k in range(len(sentence[i])):\n",
        "          if len(sentence[i][k])>8 and sentence[i][k][6]== chiffre and 'nsubj'in sentence[i][k][7] and sentence[i][k][0] not in lis:\n",
        "            lis.append(sentence[i][k][0])\n",
        "            num += int(chiffre)-int(sentence[i][k][0])\n",
        "            deno+=1\n",
        "  return (num/deno)\n",
        "\n",
        "print(distance_verbe('RNN_UniversalDependencies_FR/Donnees/fr_sequoia-ud-train.conllu.txt'))\n",
        "print(distance_verbe('RNN_UniversalDependencies_FR/Donnees/fr_sequoia-ud-test.conllu.txt'))\n",
        "         "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4.508871989860583\n",
            "4.569131832797428\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLxHtEhK81l9",
        "colab_type": "code",
        "outputId": "ec5febb3-53ce-4c99-ea5f-d377face459f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def donne(fic):\n",
        "  print(\"lecture des donnees d'entrée depuis le fichier : \", fic) \n",
        "  try:\n",
        "    fichier = open(fic,\"r\")\n",
        "  except IOError:\n",
        "    print(\"le fichier\",fic, \"n'existe pas\")\n",
        "    return None\n",
        "  \n",
        "  liste=[]\n",
        "  for ligne in fichier :  \n",
        "    ligne = ligne.strip('\\n\\r') \n",
        "    seq = [x for x in ligne.split()]  \n",
        "    liste.append(seq)  \n",
        "  fichier.close()\n",
        "  \n",
        "  sentence = []\n",
        "  part=[]\n",
        "  for l in range (len(liste)-1):\n",
        "    if len(liste[l])!= 0:\n",
        "      part.append(liste[l])\n",
        "    else:\n",
        "      sentence.append(part)\n",
        "      part=[]\n",
        "\n",
        "  POP=[]\n",
        "  Out=[]\n",
        "  for i in range(len(sentence)):  \n",
        "    pa=[]\n",
        "    no=[]\n",
        "    if i <= len(sentence)//2:\n",
        "      for e in range(2,len(sentence[i])):\n",
        "        flag=0\n",
        "        inf=''\n",
        "        nomb=''\n",
        "        if sentence[i][e][3] == \"AUX\":\n",
        "          if sentence[i][e][2] == \"avoir\":\n",
        "            inf+='_avoir'\n",
        "          else:\n",
        "            inf+='_etre'\n",
        "\n",
        "        if \"Sing\" in sentence[i][e][5]:\n",
        "          nomb+=\"_S\"\n",
        "        elif \"Plur\" in sentence[i][e][5]:\n",
        "          nomb+=\"_P\"\n",
        "\n",
        "        pa.append(sentence[i][e][3]+inf+nomb)\n",
        "        if sentence[i][e][3] != \"VERB\":\n",
        "          no.append(3)\n",
        "        if sentence[i][e][3] == \"VERB\" and len(nomb) != 0:\n",
        "          no.append(2)\n",
        "        if sentence[i][e][3] == \"VERB\" and len(nomb) == 0:\n",
        "          no.append(3)\n",
        "      POP.append(pa)\n",
        "      Out.append(no)\n",
        "      \n",
        "    else:\n",
        "      for e in range(2,len(sentence[i])):\n",
        "        flag=0\n",
        "        inf=''\n",
        "        nomb=''\n",
        "        if sentence[i][e][3] == \"AUX\":\n",
        "          if sentence[i][e][2] == \"avoir\":\n",
        "            inf+='_avoir'\n",
        "          else:\n",
        "            inf+='_etre'\n",
        "            \n",
        "        if sentence[i][e][3] != \"VERB\":\n",
        "          if \"Sing\" in sentence[i][e][5]:\n",
        "            nomb+=\"_S\"\n",
        "          elif \"Plur\" in sentence[i][e][5]:\n",
        "            nomb+=\"_P\"\n",
        "          pa.append(sentence[i][e][3]+inf+nomb)\n",
        "          no.append(3)\n",
        "        elif sentence[i][e][3] == \"VERB\":\n",
        "          if \"Sing\" in sentence[i][e][5]:\n",
        "            nomb+=\"_P\"\n",
        "          elif \"Plur\" in sentence[i][e][5]:\n",
        "            nomb+=\"_S\"\n",
        "          pa.append(sentence[i][e][3]+inf+nomb)\n",
        "          if len(nomb) != 0:\n",
        "            no.append(1)\n",
        "          else: #elif len(nomb) == 0:\n",
        "            no.append(3)\n",
        "           \n",
        "      POP.append(pa)\n",
        "      Out.append(no)\n",
        "  return(POP,Out) \n",
        "\n",
        "\n",
        "\n",
        "def maximun(POP_tr,POP_te):\n",
        "  maxi = len(POP_tr[0])\n",
        "  for i in POP_tr:\n",
        "    if len(i) > maxi:\n",
        "      maxi=len(i)\n",
        "  for j in POP_te:\n",
        "    if len(j) > maxi:\n",
        "      maxi=len(j)\n",
        "  return(maxi)\n",
        "\n",
        "def dico(POP_tr,POP_te):\n",
        "  dic={}\n",
        "  lis=[]\n",
        "  k=1\n",
        "  for i in range (len(POP_tr)):\n",
        "    for j in range(len(POP_tr[i])):\n",
        "      if POP_tr[i][j] not in lis:\n",
        "        lis.append(POP_tr[i][j])\n",
        "        dic[POP_tr[i][j]]=k\n",
        "        k+=1\n",
        "        \n",
        "  for i in range (len(POP_te)):\n",
        "    for j in range(len(POP_te[i])):\n",
        "      if POP_te[i][j] not in lis:\n",
        "        lis.append(POP_te[i][j])\n",
        "        dic[POP_te[i][j]]=k\n",
        "        k+=1  \n",
        "  return(dic)\n",
        "\n",
        "def convert(POP,dic):\n",
        "  Input=[]\n",
        "  for i in range (len(POP)):\n",
        "    intp=[]\n",
        "    for j in range(len(POP[i])):\n",
        "      intp.append(dic[POP[i][j]])\n",
        "    Input.append(intp)\n",
        "  return(Input)\n",
        "\n",
        "\n",
        "def rempli(POP_tr,POP_te,Out_tr,Out_te,maxi):\n",
        "  \n",
        "  for i in range (len(POP_tr)):\n",
        "    if len(POP_tr[i]) < maxi:\n",
        "      for k in range(maxi-len(POP_tr[i])):\n",
        "        POP_tr[i].insert(0,0)\n",
        "        Out_tr[i].insert(0,0)\n",
        "  \n",
        "  for j in range (len(POP_te)):\n",
        "    if len(POP_te[j]) < maxi:\n",
        "      for k in range(maxi-len(POP_te[j])):\n",
        "        POP_te[j].insert(0,0)\n",
        "        Out_te[j].insert(0,0)\n",
        " \n",
        " \n",
        "  return(POP_tr,Out_tr,POP_te,Out_te)\n",
        "\n",
        "\n",
        "def mise_en_forme(Inp,Out):\n",
        "  \n",
        "  POP_r = np.array(Inp).reshape (len(Inp),maxi,1)\n",
        "  Out_r = np.array(Out).reshape (len(Out),maxi)\n",
        "  \n",
        "  return(POP_r,Out_r)\n",
        "\n",
        "\n",
        "\n",
        "inp_tr,Out_tr= donne('RNN_UniversalDependencies_FR/Donnees/fr_sequoia-ud-train.conllu.txt')\n",
        "inp_te,Out_te= donne('RNN_UniversalDependencies_FR/Donnees/fr_sequoia-ud-test.conllu.txt')\n",
        "maxi = maximun(inp_tr,inp_te)\n",
        "dictionnaire=dico(inp_tr,inp_te)\n",
        "\n",
        "x_train = convert(inp_tr,dictionnaire)\n",
        "x_test = convert(inp_te,dictionnaire)\n",
        "inp_tr_r,Out_tr_r,inp_te_r,Out_te_r= rempli(x_train,x_test,Out_tr,Out_te,maxi)\n",
        "\n",
        "x_train,y_train = mise_en_forme(inp_tr_r,Out_tr_r)\n",
        "print('x shape = ', x_train.shape)\n",
        "print('y shape = ', y_train.shape)\n",
        "\n",
        "x_test,y_test = mise_en_forme(inp_te_r,Out_te_r)\n",
        "print('x shape = ', x_test.shape)\n",
        "print('y shape = ', y_test.shape)\n",
        "\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "lecture des donnees d'entrée depuis le fichier :  RNN_UniversalDependencies_FR/Donnees/fr_sequoia-ud-train.conllu.txt\n",
            "lecture des donnees d'entrée depuis le fichier :  RNN_UniversalDependencies_FR/Donnees/fr_sequoia-ud-test.conllu.txt\n",
            "x shape =  (2230, 150, 1)\n",
            "y shape =  (2230, 150)\n",
            "x shape =  (455, 150, 1)\n",
            "y shape =  (455, 150)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rK2PI8xWAVOE",
        "colab_type": "code",
        "outputId": "6f35336e-5558-4361-c63b-ba72a81e4e44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "Verbe_correct = 0\n",
        "Verbe_faux = 0\n",
        "indice_Vc=[]\n",
        "indice_Vf=[]\n",
        "for i in range (len(y_train)):\n",
        "  ivc=[]\n",
        "  ivf=[]\n",
        "  for j in range(len(y_train[i])):\n",
        "    if y_train[i][j] == 1:\n",
        "      Verbe_faux += 1\n",
        "      ivf.append(j)\n",
        "    elif y_train[i][j] == 2:\n",
        "      Verbe_correct += 1\n",
        "      ivc.append(j)\n",
        "  indice_Vc.append(ivc)\n",
        "  indice_Vf.append(ivf)\n",
        "\n",
        "print(Verbe_correct)\n",
        "print(Verbe_faux)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1388\n",
            "1682\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uX0wIn3diiL",
        "colab_type": "code",
        "outputId": "8f42c594-5b9e-48e3-ab43-2684be7ac272",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3931
        }
      },
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import TimeDistributed\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model = Sequential()  \n",
        "model.add(LSTM(150,input_shape=(150,1),return_sequences=False,dropout=0.2, recurrent_dropout=0.2))\n",
        "#model.add(TimeDistributed(Dense(activation='linear',units=1)))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
        "model.fit(x_train,y_train,epochs=100,batch_size=100,validation_data=(x_test, y_test))\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 2230 samples, validate on 455 samples\n",
            "Epoch 1/100\n",
            "2230/2230 [==============================] - 14s 6ms/step - loss: 3.2765 - acc: 0.7463 - val_loss: 2.9103 - val_acc: 0.6435\n",
            "Epoch 2/100\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: 0.9177 - acc: 0.4475 - val_loss: -0.5341 - val_acc: 0.2916\n",
            "Epoch 3/100\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -0.8701 - acc: 0.3070 - val_loss: -1.3803 - val_acc: 0.2885\n",
            "Epoch 4/100\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -1.6804 - acc: 0.3345 - val_loss: -1.9923 - val_acc: 0.3670\n",
            "Epoch 5/100\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -1.9619 - acc: 0.4155 - val_loss: -2.2362 - val_acc: 0.4410\n",
            "Epoch 6/100\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -2.1864 - acc: 0.4481 - val_loss: -2.4127 - val_acc: 0.4594\n",
            "Epoch 7/100\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -2.2695 - acc: 0.4743 - val_loss: -2.4859 - val_acc: 0.4743\n",
            "Epoch 8/100\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -2.3996 - acc: 0.4653 - val_loss: -2.5534 - val_acc: 0.4652\n",
            "Epoch 9/100\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -2.4911 - acc: 0.4765 - val_loss: -2.5754 - val_acc: 0.4670\n",
            "Epoch 10/100\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -2.5592 - acc: 0.4754 - val_loss: -2.6475 - val_acc: 0.4708\n",
            "Epoch 11/100\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -2.6478 - acc: 0.4712 - val_loss: -2.8267 - val_acc: 0.4751\n",
            "Epoch 12/100\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -2.7135 - acc: 0.4884 - val_loss: -2.8613 - val_acc: 0.4718\n",
            "Epoch 13/100\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -2.8345 - acc: 0.4724 - val_loss: -2.8655 - val_acc: 0.4615\n",
            "Epoch 14/100\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -2.8328 - acc: 0.4708 - val_loss: -2.9921 - val_acc: 0.4891\n",
            "Epoch 15/100\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -2.7071 - acc: 0.4633 - val_loss: -2.7852 - val_acc: 0.4918\n",
            "Epoch 16/100\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -2.7800 - acc: 0.4945 - val_loss: -2.9298 - val_acc: 0.4996\n",
            "Epoch 17/100\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -2.8697 - acc: 0.5013 - val_loss: -2.9663 - val_acc: 0.5034\n",
            "Epoch 18/100\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -2.9454 - acc: 0.4977 - val_loss: -3.0389 - val_acc: 0.5037\n",
            "Epoch 19/100\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -2.9923 - acc: 0.5019 - val_loss: -3.0676 - val_acc: 0.5035\n",
            "Epoch 20/100\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -2.9199 - acc: 0.5115 - val_loss: -3.0919 - val_acc: 0.5093\n",
            "Epoch 21/100\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -2.9897 - acc: 0.5002 - val_loss: -3.1575 - val_acc: 0.4981\n",
            "Epoch 22/100\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -3.0755 - acc: 0.4927 - val_loss: -3.1786 - val_acc: 0.4972\n",
            "Epoch 23/100\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -3.1145 - acc: 0.4960 - val_loss: -3.1498 - val_acc: 0.4908\n",
            "Epoch 24/100\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -3.1187 - acc: 0.4979 - val_loss: -3.2407 - val_acc: 0.5102\n",
            "Epoch 25/100\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -3.1452 - acc: 0.5058 - val_loss: -3.2050 - val_acc: 0.5025\n",
            "Epoch 26/100\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -3.0704 - acc: 0.5090 - val_loss: -3.1831 - val_acc: 0.5110\n",
            "Epoch 27/100\n",
            "2230/2230 [==============================] - 12s 6ms/step - loss: -3.1115 - acc: 0.5030 - val_loss: -3.1851 - val_acc: 0.4964\n",
            "Epoch 28/100\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -3.1991 - acc: 0.5031 - val_loss: -3.2165 - val_acc: 0.4985\n",
            "Epoch 29/100\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -3.2286 - acc: 0.5060 - val_loss: -3.2903 - val_acc: 0.5085\n",
            "Epoch 30/100\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -3.2519 - acc: 0.5075 - val_loss: -3.2585 - val_acc: 0.5041\n",
            "Epoch 31/100\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -3.2594 - acc: 0.5029 - val_loss: -3.3115 - val_acc: 0.5046\n",
            "Epoch 32/100\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -3.2783 - acc: 0.5039 - val_loss: -3.3234 - val_acc: 0.5031\n",
            "Epoch 33/100\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -3.3000 - acc: 0.5010 - val_loss: -3.3523 - val_acc: 0.5099\n",
            "Epoch 34/100\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -3.2864 - acc: 0.5023 - val_loss: -3.3197 - val_acc: 0.5023\n",
            "Epoch 35/100\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -3.2910 - acc: 0.5017 - val_loss: -3.3490 - val_acc: 0.5087\n",
            "Epoch 36/100\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -3.3069 - acc: 0.5087 - val_loss: -3.3384 - val_acc: 0.5054\n",
            "Epoch 37/100\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -3.3204 - acc: 0.5039 - val_loss: -3.3827 - val_acc: 0.5097\n",
            "Epoch 38/100\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -3.3417 - acc: 0.5064 - val_loss: -3.4029 - val_acc: 0.5091\n",
            "Epoch 39/100\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -3.3746 - acc: 0.5057 - val_loss: -3.4151 - val_acc: 0.5067\n",
            "Epoch 40/100\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -3.2706 - acc: 0.5110 - val_loss: -3.3525 - val_acc: 0.5075\n",
            "Epoch 41/100\n",
            "2230/2230 [==============================] - 12s 6ms/step - loss: -3.3358 - acc: 0.5079 - val_loss: -3.3455 - val_acc: 0.5161\n",
            "Epoch 42/100\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -3.2755 - acc: 0.5097 - val_loss: -3.3312 - val_acc: 0.5014\n",
            "Epoch 43/100\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -3.3277 - acc: 0.5035 - val_loss: -3.3916 - val_acc: 0.5061\n",
            "Epoch 44/100\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -3.3624 - acc: 0.5081 - val_loss: -3.4358 - val_acc: 0.5127\n",
            "Epoch 45/100\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -3.2714 - acc: 0.5268 - val_loss: -3.3818 - val_acc: 0.5197\n",
            "Epoch 46/100\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -3.3733 - acc: 0.5160 - val_loss: -3.4360 - val_acc: 0.5121\n",
            "Epoch 47/100\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -3.4049 - acc: 0.5062 - val_loss: -3.4525 - val_acc: 0.5059\n",
            "Epoch 48/100\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -3.4455 - acc: 0.5053 - val_loss: -3.4824 - val_acc: 0.5094\n",
            "Epoch 49/100\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -3.4564 - acc: 0.5057 - val_loss: -3.4916 - val_acc: 0.5110\n",
            "Epoch 50/100\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -3.4593 - acc: 0.5078 - val_loss: -3.4882 - val_acc: 0.5096\n",
            "Epoch 51/100\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -3.4533 - acc: 0.5109 - val_loss: -3.4582 - val_acc: 0.5229\n",
            "Epoch 52/100\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -3.4266 - acc: 0.5125 - val_loss: -3.4729 - val_acc: 0.5129\n",
            "Epoch 53/100\n",
            "2230/2230 [==============================] - 12s 6ms/step - loss: -3.4600 - acc: 0.5152 - val_loss: -3.4831 - val_acc: 0.5150\n",
            "Epoch 54/100\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -3.4637 - acc: 0.5150 - val_loss: -3.5065 - val_acc: 0.5163\n",
            "Epoch 55/100\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -3.4643 - acc: 0.5130 - val_loss: -3.5171 - val_acc: 0.5152\n",
            "Epoch 56/100\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -3.4771 - acc: 0.5152 - val_loss: -3.5111 - val_acc: 0.5137\n",
            "Epoch 57/100\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -3.5035 - acc: 0.5112 - val_loss: -3.5162 - val_acc: 0.5127\n",
            "Epoch 58/100\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -3.3896 - acc: 0.5033 - val_loss: -3.4429 - val_acc: 0.5040\n",
            "Epoch 59/100\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -3.4824 - acc: 0.5092 - val_loss: -3.4153 - val_acc: 0.4976\n",
            "Epoch 60/100\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -3.4576 - acc: 0.4960 - val_loss: -3.4927 - val_acc: 0.5043\n",
            "Epoch 61/100\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -3.4866 - acc: 0.4995 - val_loss: -3.4978 - val_acc: 0.5058\n",
            "Epoch 62/100\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -3.5009 - acc: 0.5049 - val_loss: -3.5312 - val_acc: 0.5119\n",
            "Epoch 63/100\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -3.5131 - acc: 0.5042 - val_loss: -3.5227 - val_acc: 0.5059\n",
            "Epoch 64/100\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -3.5230 - acc: 0.5029 - val_loss: -3.5171 - val_acc: 0.5079\n",
            "Epoch 65/100\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -3.5101 - acc: 0.5028 - val_loss: -3.5280 - val_acc: 0.5078\n",
            "Epoch 66/100\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -3.5136 - acc: 0.5044 - val_loss: -3.5340 - val_acc: 0.5091\n",
            "Epoch 67/100\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -3.5259 - acc: 0.5039 - val_loss: -3.5457 - val_acc: 0.5110\n",
            "Epoch 68/100\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -3.5756 - acc: 0.5057 - val_loss: -3.5521 - val_acc: 0.5095\n",
            "Epoch 69/100\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -3.8336 - acc: 0.5019 - val_loss: -3.9439 - val_acc: 0.5059\n",
            "Epoch 70/100\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -3.9634 - acc: 0.4994 - val_loss: -3.9671 - val_acc: 0.5053\n",
            "Epoch 71/100\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -3.9570 - acc: 0.5016 - val_loss: -3.9631 - val_acc: 0.5086\n",
            "Epoch 72/100\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -3.9593 - acc: 0.5044 - val_loss: -3.9759 - val_acc: 0.5082\n",
            "Epoch 73/100\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -3.9647 - acc: 0.5036 - val_loss: -3.9505 - val_acc: 0.5059\n",
            "Epoch 74/100\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -3.8545 - acc: 0.5051 - val_loss: -3.9065 - val_acc: 0.5155\n",
            "Epoch 75/100\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -3.9392 - acc: 0.5095 - val_loss: -3.9606 - val_acc: 0.5097\n",
            "Epoch 76/100\n",
            "2230/2230 [==============================] - 12s 6ms/step - loss: -3.9574 - acc: 0.5044 - val_loss: -3.9569 - val_acc: 0.5074\n",
            "Epoch 77/100\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -3.9734 - acc: 0.5058 - val_loss: -3.9821 - val_acc: 0.5089\n",
            "Epoch 78/100\n",
            "2230/2230 [==============================] - 12s 6ms/step - loss: -3.8866 - acc: 0.5148 - val_loss: -3.8399 - val_acc: 0.5178\n",
            "Epoch 79/100\n",
            "2230/2230 [==============================] - 12s 6ms/step - loss: -3.7658 - acc: 0.5250 - val_loss: -3.9407 - val_acc: 0.5198\n",
            "Epoch 80/100\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -3.9285 - acc: 0.5175 - val_loss: -3.9635 - val_acc: 0.5195\n",
            "Epoch 81/100\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -3.9427 - acc: 0.5121 - val_loss: -3.9277 - val_acc: 0.5245\n",
            "Epoch 82/100\n",
            "2230/2230 [==============================] - 12s 6ms/step - loss: -1.9675 - acc: 0.4176 - val_loss: -1.6753 - val_acc: 0.4251\n",
            "Epoch 83/100\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -1.7316 - acc: 0.4352 - val_loss: -1.8170 - val_acc: 0.4238\n",
            "Epoch 84/100\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -1.7362 - acc: 0.4363 - val_loss: -1.9344 - val_acc: 0.4365\n",
            "Epoch 85/100\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -1.9450 - acc: 0.4408 - val_loss: -1.9099 - val_acc: 0.4377\n",
            "Epoch 86/100\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -1.9509 - acc: 0.4452 - val_loss: -1.9339 - val_acc: 0.4574\n",
            "Epoch 87/100\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -1.8411 - acc: 0.4538 - val_loss: -1.9870 - val_acc: 0.4609\n",
            "Epoch 88/100\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -1.8892 - acc: 0.4599 - val_loss: -1.9461 - val_acc: 0.4524\n",
            "Epoch 89/100\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -1.9952 - acc: 0.4551 - val_loss: -2.0896 - val_acc: 0.4488\n",
            "Epoch 90/100\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -2.0667 - acc: 0.4562 - val_loss: -2.0869 - val_acc: 0.4609\n",
            "Epoch 91/100\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -2.1278 - acc: 0.4618 - val_loss: -2.3229 - val_acc: 0.4658\n",
            "Epoch 92/100\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -2.1866 - acc: 0.4615 - val_loss: -2.3738 - val_acc: 0.4718\n",
            "Epoch 93/100\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -1.1729 - acc: 0.4729 - val_loss: -1.0027 - val_acc: 0.4702\n",
            "Epoch 94/100\n",
            "2230/2230 [==============================] - 12s 6ms/step - loss: -1.2483 - acc: 0.4695 - val_loss: -1.3448 - val_acc: 0.4674\n",
            "Epoch 95/100\n",
            "2230/2230 [==============================] - 12s 6ms/step - loss: -1.5315 - acc: 0.4643 - val_loss: -1.5712 - val_acc: 0.4718\n",
            "Epoch 96/100\n",
            "2230/2230 [==============================] - 12s 6ms/step - loss: -1.7636 - acc: 0.4560 - val_loss: -1.8205 - val_acc: 0.4451\n",
            "Epoch 97/100\n",
            "2230/2230 [==============================] - 12s 6ms/step - loss: -1.9058 - acc: 0.4591 - val_loss: -2.0037 - val_acc: 0.4613\n",
            "Epoch 98/100\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -1.9971 - acc: 0.4632 - val_loss: -2.0487 - val_acc: 0.4574\n",
            "Epoch 99/100\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -2.0710 - acc: 0.4704 - val_loss: -2.1299 - val_acc: 0.4705\n",
            "Epoch 100/100\n",
            "2230/2230 [==============================] - 12s 6ms/step - loss: -2.0906 - acc: 0.4689 - val_loss: -2.1558 - val_acc: 0.4596\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f4a56b225f8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPuIoYQZxmth",
        "colab_type": "code",
        "outputId": "94b5842c-4347-4174-93c8-f4dc54a10244",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "l=model.predict(x_train)\n",
        "vcnp = 0\n",
        "vfnp = 0\n",
        "for i in range(len(l)):\n",
        "  for j in range(len(l[i])):\n",
        "    for k in range (len(indice_Vc[i])):\n",
        "      if indice_Vc[i][k] == j:\n",
        "        if int(l[i][j]) == 2:\n",
        "          vcnp +=1\n",
        "    for p in range (len(indice_Vf[i])):\n",
        "      if indice_Vf[i][p] == j:\n",
        "        if int(l[i][j]) == 1:\n",
        "          vfnp +=1\n",
        "print(vcnp)\n",
        "print(vfnp)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "1340\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}