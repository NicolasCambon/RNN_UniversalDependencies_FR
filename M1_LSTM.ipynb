{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "M1_LSTM.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rc7DMjd1uPpB",
        "colab_type": "code",
        "outputId": "8427cd71-78a4-48fc-bca6-a6183dd2b14f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!git clone https://github.com/NicolasCambon/RNN_UniversalDependencies_FR.git #Téléchargment des données"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'RNN_UniversalDependencies_FR' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5NiFTb58tA58",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "19bdb199-cfdf-490e-8dec-ca3a229a634a"
      },
      "source": [
        "def distance_verbe(fic):\n",
        "  try:\n",
        "    fichier = open(fic,\"r\")\n",
        "  except IOError:\n",
        "    print(\"le fichier\",fic, \"n'existe pas\")\n",
        "    return None\n",
        "  \n",
        "  liste=[]\n",
        "  for ligne in fichier :  \n",
        "    ligne = ligne.strip('\\n\\r') \n",
        "    seq = [x for x in ligne.split()]  \n",
        "    liste.append(seq)  \n",
        "  fichier.close()\n",
        "  \n",
        "  sentence = []\n",
        "  part=[]\n",
        "  for l in range (len(liste)-1):\n",
        "    if len(liste[l])!= 0:\n",
        "      part.append(liste[l])\n",
        "    else:\n",
        "      sentence.append(part)\n",
        "      part=[]\n",
        "  num=0\n",
        "  deno=0\n",
        "  for i in range (len(sentence)):\n",
        "    lis=[]\n",
        "    for j in range(len(sentence[i])):\n",
        "      if len(sentence[i][j])>8 and sentence[i][j][7]== 'root':\n",
        "        chiffre = sentence[i][j][0]\n",
        "        for k in range(len(sentence[i])):\n",
        "          if len(sentence[i][k])>8 and sentence[i][k][6]== chiffre and 'nsubj'in sentence[i][k][7] and sentence[i][k][0] not in lis:\n",
        "            lis.append(sentence[i][k][0])\n",
        "            num += int(chiffre)-int(sentence[i][k][0])\n",
        "            deno+=1\n",
        "  return (num/deno)\n",
        "\n",
        "print(distance_verbe('RNN_UniversalDependencies_FR/Donnees/fr_sequoia-ud-train.conllu.txt'))\n",
        "print(distance_verbe('RNN_UniversalDependencies_FR/Donnees/fr_sequoia-ud-test.conllu.txt'))\n",
        "         "
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4.508871989860583\n",
            "4.569131832797428\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLxHtEhK81l9",
        "colab_type": "code",
        "outputId": "99cbacf3-5aa5-4226-8a8c-2463b66a56f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def donne(fic):\n",
        "  print(\"lecture des donnees d'entrée depuis le fichier : \", fic) \n",
        "  try:\n",
        "    fichier = open(fic,\"r\")\n",
        "  except IOError:\n",
        "    print(\"le fichier\",fic, \"n'existe pas\")\n",
        "    return None\n",
        "  \n",
        "  liste=[]\n",
        "  for ligne in fichier :  \n",
        "    ligne = ligne.strip('\\n\\r') \n",
        "    seq = [x for x in ligne.split()]  \n",
        "    liste.append(seq)  \n",
        "  fichier.close()\n",
        "  \n",
        "  sentence = []\n",
        "  part=[]\n",
        "  for l in range (len(liste)-1):\n",
        "    if len(liste[l])!= 0:\n",
        "      part.append(liste[l])\n",
        "    else:\n",
        "      sentence.append(part)\n",
        "      part=[]\n",
        "\n",
        "  POP=[]\n",
        "  Out=[]\n",
        "  for i in range(len(sentence)):  \n",
        "    pa=[]\n",
        "    no=[]\n",
        "    if i <= len(sentence)//2:\n",
        "      for e in range(2,len(sentence[i])):\n",
        "        flag=0\n",
        "        inf=''\n",
        "        nomb=''\n",
        "        if sentence[i][e][3] == \"AUX\":\n",
        "          if sentence[i][e][2] == \"avoir\":\n",
        "            inf+='_avoir'\n",
        "          else:\n",
        "            inf+='_etre'\n",
        "\n",
        "        if \"Sing\" in sentence[i][e][5]:\n",
        "          nomb+=\"_S\"\n",
        "        elif \"Plur\" in sentence[i][e][5]:\n",
        "          nomb+=\"_P\"\n",
        "\n",
        "        pa.append(sentence[i][e][3]+inf+nomb)\n",
        "        if sentence[i][e][3] != \"VERB\":\n",
        "          no.append(\"2\")\n",
        "        if sentence[i][e][3] == \"VERB\" and len(nomb) != 0:\n",
        "          no.append(\"1\")\n",
        "        if sentence[i][e][3] == \"VERB\" and len(nomb) == 0:\n",
        "          no.append(\"2\")\n",
        "      POP.append(pa)\n",
        "      Out.append(no)\n",
        "      \n",
        "    else:\n",
        "      for e in range(2,len(sentence[i])):\n",
        "        flag=0\n",
        "        inf=''\n",
        "        nomb=''\n",
        "        if sentence[i][e][3] == \"AUX\":\n",
        "          if sentence[i][e][2] == \"avoir\":\n",
        "            inf+='_avoir'\n",
        "          else:\n",
        "            inf+='_etre'\n",
        "            \n",
        "        if sentence[i][e][3] != \"VERB\":\n",
        "          if \"Sing\" in sentence[i][e][5]:\n",
        "            nomb+=\"_S\"\n",
        "          elif \"Plur\" in sentence[i][e][5]:\n",
        "            nomb+=\"_P\"\n",
        "          pa.append(sentence[i][e][3]+inf+nomb)\n",
        "          no.append(\"2\")\n",
        "        elif sentence[i][e][3] == \"VERB\":\n",
        "          if \"Sing\" in sentence[i][e][5]:\n",
        "            nomb+=\"_P\"\n",
        "          elif \"Plur\" in sentence[i][e][5]:\n",
        "            nomb+=\"_S\"\n",
        "          pa.append(sentence[i][e][3]+inf+nomb)\n",
        "          if len(nomb) != 0:\n",
        "            no.append(\"0\")\n",
        "          else: #elif len(nomb) == 0:\n",
        "            no.append(\"2\")\n",
        "           \n",
        "      POP.append(pa)\n",
        "      Out.append(no)\n",
        "  return(POP,Out) \n",
        "\n",
        "\n",
        "def maximun(POP_tr,POP_te):\n",
        "  maxi = len(POP_tr[0])\n",
        "  for i in POP_tr:\n",
        "    if len(i) > maxi:\n",
        "      maxi=len(i)\n",
        "  for j in POP_te:\n",
        "    if len(j) > maxi:\n",
        "      maxi=len(j)\n",
        "  return(maxi)\n",
        "\n",
        "def rempli(POP_tr,POP_te,Out_tr,Out_te,maxi):\n",
        "  \n",
        "  for i in range (len(POP_tr)):\n",
        "    if len(POP_tr[i]) < maxi:\n",
        "      for k in range(maxi-len(POP_tr[i])):\n",
        "        POP_tr[i].append('Remplisseur')\n",
        "        Out_tr[i].append(2)\n",
        "  \n",
        "  for j in range (len(POP_te)):\n",
        "    if len(POP_te[j]) < maxi:\n",
        "      for k in range(maxi-len(POP_te[j])):\n",
        "        POP_te[j].append('Remplisseur')\n",
        "        Out_te[j].append(2)\n",
        "        \n",
        "  return(POP_tr,Out_tr,POP_te,Out_te)\n",
        "\n",
        "def dico(POP_tr,POP_te):\n",
        "  dic={}\n",
        "  lis=[]\n",
        "  k=1\n",
        "  for i in range (len(POP_tr)):\n",
        "    for j in range(len(POP_tr[i])):\n",
        "      if POP_tr[i][j] not in lis:\n",
        "        lis.append(POP_tr[i][j])\n",
        "        dic[POP_tr[i][j]]=k\n",
        "        k+=1\n",
        "        \n",
        "  for i in range (len(POP_te)):\n",
        "    for j in range(len(POP_te[i])):\n",
        "      if POP_te[i][j] not in lis:\n",
        "        lis.append(POP_te[i][j])\n",
        "        dic[POP_te[i][j]]=k\n",
        "        k+=1  \n",
        "  return(dic)\n",
        "\n",
        "def mise_en_forme(POP,Out,dic):\n",
        "  Input=[]\n",
        "  for i in range (len(POP)):\n",
        "    intp=[]\n",
        "    for j in range(len(POP[i])):\n",
        "      intp.append(dic[POP[i][j]])\n",
        "    Input.append(intp)\n",
        "  \n",
        "  POP_r = np.array(Input).reshape (len(Input),maxi,1)\n",
        "  Out_r = np.array(Out).reshape (len(Out),maxi,1)\n",
        "  return(POP_r,Out_r)\n",
        "\n",
        "\n",
        "inp_tr,Out_tr= donne('RNN_UniversalDependencies_FR/Donnees/fr_sequoia-ud-train.conllu.txt')\n",
        "inp_te,Out_te= donne('RNN_UniversalDependencies_FR/Donnees/fr_sequoia-ud-test.conllu.txt')\n",
        "maxi = maximun(inp_tr,inp_te)\n",
        "inp_tr_r,Out_tr_r,inp_te_r,Out_te_r= rempli(inp_tr,inp_te,Out_tr,Out_te,maxi)\n",
        "dictionnaire=dico(inp_tr_r,inp_te_r)\n",
        "\n",
        "\n",
        "\n",
        "x_train,y_train = mise_en_forme(inp_tr_r,Out_tr_r,dictionnaire)\n",
        "print('x shape = ', x_train.shape)\n",
        "print('y shape = ', y_train.shape)\n",
        "x_test,y_test = mise_en_forme(inp_te_r,Out_te_r,dictionnaire)\n",
        "print('x shape = ', x_test.shape)\n",
        "print('y shape = ', y_test.shape)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "lecture des donnees d'entrée depuis le fichier :  RNN_UniversalDependencies_FR/Donnees/fr_sequoia-ud-train.conllu.txt\n",
            "lecture des donnees d'entrée depuis le fichier :  RNN_UniversalDependencies_FR/Donnees/fr_sequoia-ud-test.conllu.txt\n",
            "x shape =  (2230, 150, 1)\n",
            "y shape =  (2230, 150, 1)\n",
            "x shape =  (455, 150, 1)\n",
            "y shape =  (455, 150, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uX0wIn3diiL",
        "colab_type": "code",
        "outputId": "e9f2ce93-9594-4c33-9d27-3c18b6334409",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        }
      },
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import TimeDistributed\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model = Sequential()  \n",
        "model.add(LSTM(maxi,input_shape=(x_train.shape[1:]),return_sequences=True))\n",
        "model.add(TimeDistributed(Dense(activation='linear',units=1)))\n",
        "model.compile(loss='mean_squared_error', optimizer='adam',metrics=['accuracy'])\n",
        "model.fit(x_train,y_train,epochs=10,batch_size=128,validation_data=(x_test, y_test))\n"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 2230 samples, validate on 455 samples\n",
            "Epoch 1/10\n",
            "2230/2230 [==============================] - 13s 6ms/step - loss: 0.9615 - acc: 0.4996 - val_loss: 0.1813 - val_acc: 0.9106\n",
            "Epoch 2/10\n",
            "2230/2230 [==============================] - 11s 5ms/step - loss: 0.1085 - acc: 0.9630 - val_loss: 0.0636 - val_acc: 0.9668\n",
            "Epoch 3/10\n",
            "2230/2230 [==============================] - 11s 5ms/step - loss: 0.0565 - acc: 0.9664 - val_loss: 0.0443 - val_acc: 0.9744\n",
            "Epoch 4/10\n",
            "2230/2230 [==============================] - 11s 5ms/step - loss: 0.0467 - acc: 0.9745 - val_loss: 0.0403 - val_acc: 0.9763\n",
            "Epoch 5/10\n",
            "2230/2230 [==============================] - 11s 5ms/step - loss: 0.0438 - acc: 0.9756 - val_loss: 0.0383 - val_acc: 0.9765\n",
            "Epoch 6/10\n",
            "2230/2230 [==============================] - 11s 5ms/step - loss: 0.0421 - acc: 0.9758 - val_loss: 0.0368 - val_acc: 0.9769\n",
            "Epoch 7/10\n",
            "2230/2230 [==============================] - 11s 5ms/step - loss: 0.0408 - acc: 0.9767 - val_loss: 0.0355 - val_acc: 0.9783\n",
            "Epoch 8/10\n",
            "2230/2230 [==============================] - 11s 5ms/step - loss: 0.0396 - acc: 0.9780 - val_loss: 0.0343 - val_acc: 0.9795\n",
            "Epoch 9/10\n",
            "2230/2230 [==============================] - 11s 5ms/step - loss: 0.0385 - acc: 0.9790 - val_loss: 0.0333 - val_acc: 0.9804\n",
            "Epoch 10/10\n",
            "2230/2230 [==============================] - 11s 5ms/step - loss: 0.0375 - acc: 0.9796 - val_loss: 0.0323 - val_acc: 0.9808\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f9d77dfa4a8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPuIoYQZxmth",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5527
        },
        "outputId": "d2bbfffc-fbbb-4a66-ef8b-0d914c711b76"
      },
      "source": [
        "print(y_train[2000:2001])\n",
        "model.predict(x_train[2000:2001])"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['0']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['0']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['0']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']\n",
            "  ['2']]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[1.1120003],\n",
              "        [1.6399295],\n",
              "        [1.8926003],\n",
              "        [1.9465195],\n",
              "        [2.0262365],\n",
              "        [2.031356 ],\n",
              "        [2.0674906],\n",
              "        [2.0495257],\n",
              "        [2.013738 ],\n",
              "        [2.0079327],\n",
              "        [1.8370607],\n",
              "        [1.8568451],\n",
              "        [1.8502327],\n",
              "        [1.9376485],\n",
              "        [1.9871862],\n",
              "        [1.9300761],\n",
              "        [1.8590273],\n",
              "        [1.9447536],\n",
              "        [1.9899001],\n",
              "        [2.0031924],\n",
              "        [2.0113032],\n",
              "        [1.9932808],\n",
              "        [1.9262164],\n",
              "        [1.843212 ],\n",
              "        [1.8572915],\n",
              "        [1.8874843],\n",
              "        [1.7505143],\n",
              "        [1.8114134],\n",
              "        [1.9362009],\n",
              "        [1.9465799],\n",
              "        [1.9421171],\n",
              "        [1.9897081],\n",
              "        [1.9946744],\n",
              "        [1.9944253],\n",
              "        [1.9914556],\n",
              "        [1.9877791],\n",
              "        [1.984286 ],\n",
              "        [1.9814689],\n",
              "        [1.9796139],\n",
              "        [1.9788399],\n",
              "        [1.9791108],\n",
              "        [1.9802654],\n",
              "        [1.9820671],\n",
              "        [1.9842569],\n",
              "        [1.9865988],\n",
              "        [1.9889067],\n",
              "        [1.9910538],\n",
              "        [1.9929641],\n",
              "        [1.9946066],\n",
              "        [1.9959797],\n",
              "        [1.9971008],\n",
              "        [1.9979979],\n",
              "        [1.9987029],\n",
              "        [1.9992466],\n",
              "        [1.9996599],\n",
              "        [1.9999678],\n",
              "        [2.000193 ],\n",
              "        [2.0003545],\n",
              "        [2.000466 ],\n",
              "        [2.0005412],\n",
              "        [2.000589 ],\n",
              "        [2.0006166],\n",
              "        [2.0006297],\n",
              "        [2.0006335],\n",
              "        [2.0006301],\n",
              "        [2.0006227],\n",
              "        [2.000613 ],\n",
              "        [2.0006018],\n",
              "        [2.0005903],\n",
              "        [2.000579 ],\n",
              "        [2.0005684],\n",
              "        [2.0005581],\n",
              "        [2.0005493],\n",
              "        [2.0005412],\n",
              "        [2.000534 ],\n",
              "        [2.0005276],\n",
              "        [2.0005224],\n",
              "        [2.0005174],\n",
              "        [2.0005133],\n",
              "        [2.0005095],\n",
              "        [2.0005069],\n",
              "        [2.000504 ],\n",
              "        [2.0005019],\n",
              "        [2.0005   ],\n",
              "        [2.0004988],\n",
              "        [2.0004973],\n",
              "        [2.0004961],\n",
              "        [2.0004954],\n",
              "        [2.0004945],\n",
              "        [2.000494 ],\n",
              "        [2.0004933],\n",
              "        [2.000493 ],\n",
              "        [2.0004926],\n",
              "        [2.0004923],\n",
              "        [2.000492 ],\n",
              "        [2.0004916],\n",
              "        [2.0004914],\n",
              "        [2.0004914],\n",
              "        [2.0004914],\n",
              "        [2.0004914],\n",
              "        [2.000491 ],\n",
              "        [2.000491 ],\n",
              "        [2.000491 ],\n",
              "        [2.000491 ],\n",
              "        [2.0004907],\n",
              "        [2.0004907],\n",
              "        [2.000491 ],\n",
              "        [2.0004907],\n",
              "        [2.0004907],\n",
              "        [2.000491 ],\n",
              "        [2.0004907],\n",
              "        [2.0004904],\n",
              "        [2.0004907],\n",
              "        [2.0004907],\n",
              "        [2.0004907],\n",
              "        [2.0004907],\n",
              "        [2.0004907],\n",
              "        [2.0004907],\n",
              "        [2.0004907],\n",
              "        [2.0004907],\n",
              "        [2.0004907],\n",
              "        [2.0004907],\n",
              "        [2.0004907],\n",
              "        [2.0004907],\n",
              "        [2.0004907],\n",
              "        [2.0004907],\n",
              "        [2.0004907],\n",
              "        [2.0004907],\n",
              "        [2.0004907],\n",
              "        [2.0004907],\n",
              "        [2.0004907],\n",
              "        [2.0004907],\n",
              "        [2.0004907],\n",
              "        [2.0004907],\n",
              "        [2.0004907],\n",
              "        [2.0004907],\n",
              "        [2.0004907],\n",
              "        [2.0004907],\n",
              "        [2.0004907],\n",
              "        [2.0004907],\n",
              "        [2.0004907],\n",
              "        [2.0004907],\n",
              "        [2.0004907],\n",
              "        [2.0004907],\n",
              "        [2.0004907],\n",
              "        [2.0004907],\n",
              "        [2.0004907],\n",
              "        [2.0004907],\n",
              "        [2.0004907],\n",
              "        [2.0004907]]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    }
  ]
}