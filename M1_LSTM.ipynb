{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "M1_LSTM.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rc7DMjd1uPpB",
        "colab_type": "code",
        "outputId": "570cb72b-bdca-4a7f-c69e-ae36abd158bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "!git clone https://github.com/NicolasCambon/RNN_UniversalDependencies_FR.git #Téléchargment des données"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'RNN_UniversalDependencies_FR'...\n",
            "remote: Enumerating objects: 43, done.\u001b[K\n",
            "remote: Counting objects: 100% (43/43), done.\u001b[K\n",
            "remote: Compressing objects: 100% (35/35), done.\u001b[K\n",
            "remote: Total 43 (delta 5), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (43/43), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLxHtEhK81l9",
        "colab_type": "code",
        "outputId": "8fba66e1-4218-40b2-fdcf-50e361c337ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def donne(fic):\n",
        "  print(\"lecture des donnees d'entrée depuis le fichier : \", fic) \n",
        "  try:\n",
        "    fichier = open(fic,\"r\")\n",
        "  except IOError:\n",
        "    print(\"le fichier\",fic, \"n'existe pas\")\n",
        "    return None\n",
        "  \n",
        "  liste=[]\n",
        "  for ligne in fichier :  \n",
        "    ligne = ligne.strip('\\n\\r') \n",
        "    seq = [x for x in ligne.split()]  \n",
        "    liste.append(seq)  \n",
        "  fichier.close()\n",
        "  \n",
        "  sentence = []\n",
        "  part=[]\n",
        "  for l in range (len(liste)-1):\n",
        "    if len(liste[l])!= 0:\n",
        "      part.append(liste[l])\n",
        "    else:\n",
        "      sentence.append(part)\n",
        "      part=[]\n",
        "\n",
        "  POP=[]\n",
        "  Out=[]\n",
        "  for i in range(len(sentence)):  \n",
        "    pa=[]\n",
        "    no=[]\n",
        "    if i <= len(sentence)//2:\n",
        "      for e in range(2,len(sentence[i])):\n",
        "        flag=0\n",
        "        inf=''\n",
        "        nomb=''\n",
        "        if sentence[i][e][3] == \"AUX\":\n",
        "          if sentence[i][e][2] == \"avoir\":\n",
        "            inf+='_avoir'\n",
        "          else:\n",
        "            inf+='_etre'\n",
        "\n",
        "        if \"Sing\" in sentence[i][e][5]:\n",
        "          nomb+=\"_S\"\n",
        "        elif \"Plur\" in sentence[i][e][5]:\n",
        "          nomb+=\"_P\"\n",
        "\n",
        "        pa.append(sentence[i][e][3]+inf+nomb)\n",
        "        if sentence[i][e][3] != \"VERB\":\n",
        "          no.append(\"2\")\n",
        "        if sentence[i][e][3] == \"VERB\" and len(nomb) != 0:\n",
        "          no.append(\"1\")\n",
        "        if sentence[i][e][3] == \"VERB\" and len(nomb) == 0:\n",
        "          no.append(\"2\")\n",
        "      POP.append(pa)\n",
        "      Out.append(no)\n",
        "      \n",
        "    else:\n",
        "      for e in range(2,len(sentence[i])):\n",
        "        flag=0\n",
        "        inf=''\n",
        "        nomb=''\n",
        "        if sentence[i][e][3] == \"AUX\":\n",
        "          if sentence[i][e][2] == \"avoir\":\n",
        "            inf+='_avoir'\n",
        "          else:\n",
        "            inf+='_etre'\n",
        "            \n",
        "        if sentence[i][e][3] != \"VERB\":\n",
        "          if \"Sing\" in sentence[i][e][5]:\n",
        "            nomb+=\"_S\"\n",
        "          elif \"Plur\" in sentence[i][e][5]:\n",
        "            nomb+=\"_P\"\n",
        "          pa.append(sentence[i][e][3]+inf+nomb)\n",
        "          no.append(\"2\")\n",
        "        elif sentence[i][e][3] == \"VERB\":\n",
        "          if \"Sing\" in sentence[i][e][5]:\n",
        "            nomb+=\"_P\"\n",
        "          elif \"Plur\" in sentence[i][e][5]:\n",
        "            nomb+=\"_S\"\n",
        "          pa.append(sentence[i][e][3]+inf+nomb)\n",
        "          if len(nomb) != 0:\n",
        "            no.append(\"0\")\n",
        "          else: #elif len(nomb) == 0:\n",
        "            no.append(\"2\")\n",
        "           \n",
        "      POP.append(pa)\n",
        "      Out.append(no)\n",
        "  return(POP,Out) \n",
        "\n",
        "\n",
        "def maximun(POP_tr,POP_te):\n",
        "  maxi = len(POP_tr[0])\n",
        "  for i in POP_tr:\n",
        "    if len(i) > maxi:\n",
        "      maxi=len(i)\n",
        "  for j in POP_te:\n",
        "    if len(j) > maxi:\n",
        "      maxi=len(j)\n",
        "  return(maxi)\n",
        "\n",
        "def rempli(POP_tr,POP_te,Out_tr,Out_te,maxi):\n",
        "  \n",
        "  for i in range (len(POP_tr)):\n",
        "    if len(POP_tr[i]) < maxi:\n",
        "      for k in range(maxi-len(POP_tr[i])):\n",
        "        POP_tr[i].append('Remplisseur')\n",
        "        Out_tr[i].append(2)\n",
        "  \n",
        "  for j in range (len(POP_te)):\n",
        "    if len(POP_te[j]) < maxi:\n",
        "      for k in range(maxi-len(POP_te[j])):\n",
        "        POP_te[j].append('Remplisseur')\n",
        "        Out_te[j].append(2)\n",
        "        \n",
        "  return(POP_tr,Out_tr,POP_te,Out_te)\n",
        "\n",
        "def dico(POP_tr,POP_te):\n",
        "  dic={}\n",
        "  lis=[]\n",
        "  k=1\n",
        "  for i in range (len(POP_tr)):\n",
        "    for j in range(len(POP_tr[i])):\n",
        "      if POP_tr[i][j] not in lis:\n",
        "        lis.append(POP_tr[i][j])\n",
        "        dic[POP_tr[i][j]]=k\n",
        "        k+=1\n",
        "        \n",
        "  for i in range (len(POP_te)):\n",
        "    for j in range(len(POP_te[i])):\n",
        "      if POP_te[i][j] not in lis:\n",
        "        lis.append(POP_te[i][j])\n",
        "        dic[POP_te[i][j]]=k\n",
        "        k+=1  \n",
        "  return(dic)\n",
        "\n",
        "def mise_en_forme(POP,Out,dic):\n",
        "  Input=[]\n",
        "  for i in range (len(POP)):\n",
        "    intp=[]\n",
        "    for j in range(len(POP[i])):\n",
        "      intp.append(dic[POP[i][j]])\n",
        "    Input.append(intp)\n",
        "  \n",
        "  POP_r = np.array(Input).reshape (len(Input),maxi,1)\n",
        "  Out_r = np.array(Out).reshape (len(Out),maxi)\n",
        "  return(POP_r,Out_r)\n",
        "\n",
        "\n",
        "inp_tr,Out_tr= donne('RNN_UniversalDependencies_FR/Donnees/fr_sequoia-ud-train.conllu.txt')\n",
        "inp_te,Out_te= donne('RNN_UniversalDependencies_FR/Donnees/fr_sequoia-ud-test.conllu.txt')\n",
        "maxi = maximun(inp_tr,inp_te)\n",
        "inp_tr_r,Out_tr_r,inp_te_r,Out_te_r= rempli(inp_tr,inp_te,Out_tr,Out_te,maxi)\n",
        "dictionnaire=dico(inp_tr_r,inp_te_r)\n",
        "\n",
        "\n",
        "\n",
        "x_train,y_train = mise_en_forme(inp_tr_r,Out_tr_r,dictionnaire)\n",
        "print('x shape = ', x_train.shape)\n",
        "print('y shape = ', y_train.shape)\n",
        "x_test,y_test = mise_en_forme(inp_te_r,Out_te_r,dictionnaire)\n",
        "print('x shape = ', x_test.shape)\n",
        "print('y shape = ', y_test.shape)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "lecture des donnees d'entrée depuis le fichier :  RNN_UniversalDependencies_FR/Donnees/fr_sequoia-ud-train.conllu.txt\n",
            "lecture des donnees d'entrée depuis le fichier :  RNN_UniversalDependencies_FR/Donnees/fr_sequoia-ud-test.conllu.txt\n",
            "x shape =  (2230, 150, 1)\n",
            "y shape =  (2230, 150)\n",
            "x shape =  (455, 150, 1)\n",
            "y shape =  (455, 150)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5NiFTb58tA58",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def distance_verbe(fic):\n",
        "  try:\n",
        "    fichier = open(fic,\"r\")\n",
        "  except IOError:\n",
        "    print(\"le fichier\",fic, \"n'existe pas\")\n",
        "    return None\n",
        "  \n",
        "  liste=[]\n",
        "  for ligne in fichier :  \n",
        "    ligne = ligne.strip('\\n\\r') \n",
        "    seq = [x for x in ligne.split()]  \n",
        "    liste.append(seq)  \n",
        "  fichier.close()\n",
        "  \n",
        "  sentence = []\n",
        "  part=[]\n",
        "  for l in range (len(liste)-1):\n",
        "    if len(liste[l])!= 0:\n",
        "      part.append(liste[l])\n",
        "    else:\n",
        "      sentence.append(part)\n",
        "      part=[]\n",
        "  num=0\n",
        "  deno=0\n",
        "  for i in range (len(sentence)):\n",
        "    lis=[]\n",
        "    for j in range(len(sentence[i])):\n",
        "      if len(sentence[i][j])>8 and sentence[i][j][7]== 'root':\n",
        "        chiffre = sentence[i][j][0]\n",
        "        for k in range(len(sentence[i])):\n",
        "          if len(sentence[i][k])>8 and sentence[i][k][6]== chiffre and 'nsubj'in sentence[i][k][7] and sentence[i][k][0] not in lis:\n",
        "            lis.append(sentence[i][k][0])\n",
        "            num += int(chiffre)-int(sentence[i][k][0])\n",
        "            deno+=1\n",
        "  return (num/deno)\n",
        "\n",
        "distance_tr = distance_verbe('RNN_UniversalDependencies_FR/Donnees/fr_sequoia-ud-train.conllu.txt')\n",
        "distance_te = distance_verbe('RNN_UniversalDependencies_FR/Donnees/fr_sequoia-ud-test.conllu.txt')\n",
        "         "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uX0wIn3diiL",
        "colab_type": "code",
        "outputId": "3ac503eb-347a-408b-90e3-d1e3aa824795",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        }
      },
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model = Sequential()  \n",
        "model.add(LSTM(maxi,input_shape=(x_train.shape[1:])))\n",
        "model.compile(loss='mean_squared_error', optimizer='adam',metrics=['accuracy'])\n",
        "model.fit(x_train,y_train,epochs=10,batch_size=128,validation_data=(x_test, y_test))\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 2230 samples, validate on 455 samples\n",
            "Epoch 1/10\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: 3.2736 - acc: 0.0000e+00 - val_loss: 2.3033 - val_acc: 0.0000e+00\n",
            "Epoch 2/10\n",
            "2230/2230 [==============================] - 10s 5ms/step - loss: 1.6023 - acc: 0.0000e+00 - val_loss: 1.1118 - val_acc: 0.9868\n",
            "Epoch 3/10\n",
            "2230/2230 [==============================] - 10s 5ms/step - loss: 1.0538 - acc: 0.9830 - val_loss: 1.0347 - val_acc: 0.9231\n",
            "Epoch 4/10\n",
            "2230/2230 [==============================] - 10s 5ms/step - loss: 1.0353 - acc: 0.9063 - val_loss: 1.0347 - val_acc: 0.9868\n",
            "Epoch 5/10\n",
            "2230/2230 [==============================] - 10s 5ms/step - loss: 1.0353 - acc: 0.9910 - val_loss: 1.0347 - val_acc: 0.9868\n",
            "Epoch 6/10\n",
            "2230/2230 [==============================] - 10s 5ms/step - loss: 1.0353 - acc: 0.9915 - val_loss: 1.0347 - val_acc: 0.9868\n",
            "Epoch 7/10\n",
            "2230/2230 [==============================] - 10s 5ms/step - loss: 1.0353 - acc: 0.9915 - val_loss: 1.0347 - val_acc: 0.9868\n",
            "Epoch 8/10\n",
            "2230/2230 [==============================] - 10s 5ms/step - loss: 1.0353 - acc: 0.9915 - val_loss: 1.0347 - val_acc: 0.9868\n",
            "Epoch 9/10\n",
            "2230/2230 [==============================] - 10s 5ms/step - loss: 1.0353 - acc: 0.9915 - val_loss: 1.0347 - val_acc: 0.9868\n",
            "Epoch 10/10\n",
            "2230/2230 [==============================] - 10s 5ms/step - loss: 1.0353 - acc: 0.9915 - val_loss: 1.0347 - val_acc: 0.9868\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f293ff657b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    }
  ]
}