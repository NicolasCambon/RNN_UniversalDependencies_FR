{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "M1_LSTM.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rc7DMjd1uPpB",
        "colab_type": "code",
        "outputId": "3f2b49d3-b40b-4c9f-9197-11a2680fc091",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!git clone https://github.com/NicolasCambon/RNN_UniversalDependencies_FR.git #Téléchargment des données"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'RNN_UniversalDependencies_FR' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5NiFTb58tA58",
        "colab_type": "code",
        "outputId": "a79a6b07-c193-4ff6-b883-710d5bd1d52a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "def distance_verbe(fic):\n",
        "  try:\n",
        "    fichier = open(fic,\"r\")\n",
        "  except IOError:\n",
        "    print(\"le fichier\",fic, \"n'existe pas\")\n",
        "    return None\n",
        "  \n",
        "  liste=[]\n",
        "  for ligne in fichier :  \n",
        "    ligne = ligne.strip('\\n\\r') \n",
        "    seq = [x for x in ligne.split()]  \n",
        "    liste.append(seq)  \n",
        "  fichier.close()\n",
        "  \n",
        "  sentence = []\n",
        "  part=[]\n",
        "  for l in range (len(liste)-1):\n",
        "    if len(liste[l])!= 0:\n",
        "      part.append(liste[l])\n",
        "    else:\n",
        "      sentence.append(part)\n",
        "      part=[]\n",
        "  num=0\n",
        "  deno=0\n",
        "  for i in range (len(sentence)):\n",
        "    lis=[]\n",
        "    for j in range(len(sentence[i])):\n",
        "      if len(sentence[i][j])>8 and sentence[i][j][7]== 'root':\n",
        "        chiffre = sentence[i][j][0]\n",
        "        for k in range(len(sentence[i])):\n",
        "          if len(sentence[i][k])>8 and sentence[i][k][6]== chiffre and 'nsubj'in sentence[i][k][7] and sentence[i][k][0] not in lis:\n",
        "            lis.append(sentence[i][k][0])\n",
        "            num += int(chiffre)-int(sentence[i][k][0])\n",
        "            deno+=1\n",
        "  return (num/deno)\n",
        "\n",
        "print(distance_verbe('RNN_UniversalDependencies_FR/Donnees/fr_sequoia-ud-train.conllu.txt'))\n",
        "print(distance_verbe('RNN_UniversalDependencies_FR/Donnees/fr_sequoia-ud-test.conllu.txt'))\n",
        "         "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4.508871989860583\n",
            "4.569131832797428\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLxHtEhK81l9",
        "colab_type": "code",
        "outputId": "b7fca980-e84b-4e71-9c7c-aa49cfff2333",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def donne(fic):\n",
        "  print(\"lecture des donnees d'entrée depuis le fichier : \", fic) \n",
        "  try:\n",
        "    fichier = open(fic,\"r\")\n",
        "  except IOError:\n",
        "    print(\"le fichier\",fic, \"n'existe pas\")\n",
        "    return None\n",
        "  \n",
        "  liste=[]\n",
        "  for ligne in fichier :  \n",
        "    ligne = ligne.strip('\\n\\r') \n",
        "    seq = [x for x in ligne.split()]  \n",
        "    liste.append(seq)  \n",
        "  fichier.close()\n",
        "  \n",
        "  sentence = []\n",
        "  part=[]\n",
        "  for l in range (len(liste)-1):\n",
        "    if len(liste[l])!= 0:\n",
        "      part.append(liste[l])\n",
        "    else:\n",
        "      sentence.append(part)\n",
        "      part=[]\n",
        "\n",
        "  POP=[]\n",
        "  Out=[]\n",
        "  for i in range(len(sentence)):  \n",
        "    pa=[]\n",
        "    no=[]\n",
        "    if i <= len(sentence)//2:\n",
        "      for e in range(2,len(sentence[i])):\n",
        "        flag=0\n",
        "        inf=''\n",
        "        nomb=''\n",
        "        if sentence[i][e][3] == \"AUX\":\n",
        "          if sentence[i][e][2] == \"avoir\":\n",
        "            inf+='_avoir'\n",
        "          else:\n",
        "            inf+='_etre'\n",
        "\n",
        "        if \"Sing\" in sentence[i][e][5]:\n",
        "          nomb+=\"_S\"\n",
        "        elif \"Plur\" in sentence[i][e][5]:\n",
        "          nomb+=\"_P\"\n",
        "\n",
        "        pa.append(sentence[i][e][3]+inf+nomb)\n",
        "        if sentence[i][e][3] != \"VERB\":\n",
        "          no.append(3)\n",
        "        if sentence[i][e][3] == \"VERB\" and len(nomb) != 0:\n",
        "          no.append(2)\n",
        "        if sentence[i][e][3] == \"VERB\" and len(nomb) == 0:\n",
        "          no.append(3)\n",
        "      POP.append(pa)\n",
        "      Out.append(no)\n",
        "      \n",
        "    else:\n",
        "      for e in range(2,len(sentence[i])):\n",
        "        flag=0\n",
        "        inf=''\n",
        "        nomb=''\n",
        "        if sentence[i][e][3] == \"AUX\":\n",
        "          if sentence[i][e][2] == \"avoir\":\n",
        "            inf+='_avoir'\n",
        "          else:\n",
        "            inf+='_etre'\n",
        "            \n",
        "        if sentence[i][e][3] != \"VERB\":\n",
        "          if \"Sing\" in sentence[i][e][5]:\n",
        "            nomb+=\"_S\"\n",
        "          elif \"Plur\" in sentence[i][e][5]:\n",
        "            nomb+=\"_P\"\n",
        "          pa.append(sentence[i][e][3]+inf+nomb)\n",
        "          no.append(3)\n",
        "        elif sentence[i][e][3] == \"VERB\":\n",
        "          if \"Sing\" in sentence[i][e][5]:\n",
        "            nomb+=\"_P\"\n",
        "          elif \"Plur\" in sentence[i][e][5]:\n",
        "            nomb+=\"_S\"\n",
        "          pa.append(sentence[i][e][3]+inf+nomb)\n",
        "          if len(nomb) != 0:\n",
        "            no.append(1)\n",
        "          else: #elif len(nomb) == 0:\n",
        "            no.append(3)\n",
        "           \n",
        "      POP.append(pa)\n",
        "      Out.append(no)\n",
        "  return(POP,Out) \n",
        "\n",
        "\n",
        "\n",
        "def maximun(POP_tr,POP_te):\n",
        "  maxi = len(POP_tr[0])\n",
        "  for i in POP_tr:\n",
        "    if len(i) > maxi:\n",
        "      maxi=len(i)\n",
        "  for j in POP_te:\n",
        "    if len(j) > maxi:\n",
        "      maxi=len(j)\n",
        "  return(maxi)\n",
        "\n",
        "def dico(POP_tr,POP_te):\n",
        "  dic={}\n",
        "  lis=[]\n",
        "  k=1\n",
        "  for i in range (len(POP_tr)):\n",
        "    for j in range(len(POP_tr[i])):\n",
        "      if POP_tr[i][j] not in lis:\n",
        "        lis.append(POP_tr[i][j])\n",
        "        dic[POP_tr[i][j]]=k\n",
        "        k+=1\n",
        "        \n",
        "  for i in range (len(POP_te)):\n",
        "    for j in range(len(POP_te[i])):\n",
        "      if POP_te[i][j] not in lis:\n",
        "        lis.append(POP_te[i][j])\n",
        "        dic[POP_te[i][j]]=k\n",
        "        k+=1  \n",
        "  return(dic)\n",
        "\n",
        "def convert(POP,dic):\n",
        "  Input=[]\n",
        "  for i in range (len(POP)):\n",
        "    intp=[]\n",
        "    for j in range(len(POP[i])):\n",
        "      intp.append(dic[POP[i][j]])\n",
        "    Input.append(intp)\n",
        "  return(Input)\n",
        "\n",
        "\n",
        "def rempli(POP_tr,POP_te,Out_tr,Out_te,maxi):\n",
        "  \n",
        "  for i in range (len(POP_tr)):\n",
        "    if len(POP_tr[i]) < maxi:\n",
        "      for k in range(maxi-len(POP_tr[i])):\n",
        "        POP_tr[i].append(0)\n",
        "        Out_tr[i].append(0)\n",
        "  \n",
        "  for j in range (len(POP_te)):\n",
        "    if len(POP_te[j]) < maxi:\n",
        "      for k in range(maxi-len(POP_te[j])):\n",
        "        POP_te[j].append(0)\n",
        "        Out_te[j].append(0)\n",
        " \n",
        " \n",
        "  return(POP_tr,Out_tr,POP_te,Out_te)\n",
        "\n",
        "\n",
        "def mise_en_forme(Inp,Out):\n",
        "  \n",
        "  POP_r = np.array(Inp).reshape (len(Inp),maxi,1)\n",
        "  Out_r = np.array(Out).reshape (len(Out),maxi)\n",
        "  \n",
        "  return(POP_r,Out_r)\n",
        "\n",
        "\n",
        "\n",
        "inp_tr,Out_tr= donne('RNN_UniversalDependencies_FR/Donnees/fr_sequoia-ud-train.conllu.txt')\n",
        "inp_te,Out_te= donne('RNN_UniversalDependencies_FR/Donnees/fr_sequoia-ud-test.conllu.txt')\n",
        "maxi = maximun(inp_tr,inp_te)\n",
        "dictionnaire=dico(inp_tr,inp_te)\n",
        "\n",
        "x_train = convert(inp_tr,dictionnaire)\n",
        "x_test = convert(inp_te,dictionnaire)\n",
        "inp_tr_r,Out_tr_r,inp_te_r,Out_te_r= rempli(x_train,x_test,Out_tr,Out_te,maxi)\n",
        "\n",
        "x_train,y_train = mise_en_forme(inp_tr_r,Out_tr_r)\n",
        "print('x shape = ', x_train.shape)\n",
        "print('y shape = ', y_train.shape)\n",
        "\n",
        "x_test,y_test = mise_en_forme(inp_te_r,Out_te_r)\n",
        "print('x shape = ', x_test.shape)\n",
        "print('y shape = ', y_test.shape)\n",
        "\n",
        "\n"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "lecture des donnees d'entrée depuis le fichier :  RNN_UniversalDependencies_FR/Donnees/fr_sequoia-ud-train.conllu.txt\n",
            "lecture des donnees d'entrée depuis le fichier :  RNN_UniversalDependencies_FR/Donnees/fr_sequoia-ud-test.conllu.txt\n",
            "x shape =  (2230, 150, 1)\n",
            "y shape =  (2230, 150)\n",
            "x shape =  (455, 150, 1)\n",
            "y shape =  (455, 150)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rK2PI8xWAVOE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "3acfee3a-76f6-458c-f7bf-a79dc039675e"
      },
      "source": [
        "Verbe_correct = 0\n",
        "Verbe_faux = 0\n",
        "indice_Vc=[]\n",
        "indice_Vf=[]\n",
        "for i in range (len(y_train)):\n",
        "  ivc=[]\n",
        "  ivf=[]\n",
        "  for j in range(len(y_train[i])):\n",
        "    if y_train[i][j] == 1:\n",
        "      Verbe_faux += 1\n",
        "      ivf.append(j)\n",
        "    elif y_train[i][j] == 2:\n",
        "      Verbe_correct += 1\n",
        "      ivc.append(j)\n",
        "  indice_Vc.append(ivc)\n",
        "  indice_Vf.append(ivf)\n",
        "\n",
        "print(Verbe_correct)\n",
        "print(Verbe_faux)"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1388\n",
            "1682\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uX0wIn3diiL",
        "colab_type": "code",
        "outputId": "6d02b0a1-418b-46b8-ccdd-a6876eca3fe5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1890
        }
      },
      "source": [
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import TimeDistributed\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model = Sequential()  \n",
        "model.add(LSTM(150,input_shape=(150,1),return_sequences=False,dropout=0.2, recurrent_dropout=0.2))\n",
        "#model.add(TimeDistributed(Dense(activation='linear',units=1)))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
        "model.fit(x_train,y_train,epochs=50,batch_size=100,validation_data=(x_test, y_test))\n"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 2230 samples, validate on 455 samples\n",
            "Epoch 1/50\n",
            "2230/2230 [==============================] - 16s 7ms/step - loss: 2.7804 - acc: 0.8305 - val_loss: 1.0848 - val_acc: 0.5578\n",
            "Epoch 2/50\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: 0.1675 - acc: 0.3917 - val_loss: -1.0376 - val_acc: 0.2865\n",
            "Epoch 3/50\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -1.1081 - acc: 0.3187 - val_loss: -1.0866 - val_acc: 0.3767\n",
            "Epoch 4/50\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -1.1978 - acc: 0.4259 - val_loss: -1.1320 - val_acc: 0.4484\n",
            "Epoch 5/50\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -1.2333 - acc: 0.4505 - val_loss: -1.1612 - val_acc: 0.4537\n",
            "Epoch 6/50\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -1.2741 - acc: 0.4514 - val_loss: -1.1928 - val_acc: 0.4527\n",
            "Epoch 7/50\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -1.2879 - acc: 0.4527 - val_loss: -1.2430 - val_acc: 0.4537\n",
            "Epoch 8/50\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -1.3253 - acc: 0.4521 - val_loss: -1.2484 - val_acc: 0.4537\n",
            "Epoch 9/50\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -1.3311 - acc: 0.4559 - val_loss: -1.2522 - val_acc: 0.4537\n",
            "Epoch 10/50\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -1.3321 - acc: 0.4613 - val_loss: -1.2573 - val_acc: 0.4652\n",
            "Epoch 11/50\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -1.3342 - acc: 0.4641 - val_loss: -1.2576 - val_acc: 0.4652\n",
            "Epoch 12/50\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -1.3410 - acc: 0.4667 - val_loss: -1.2604 - val_acc: 0.4652\n",
            "Epoch 13/50\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -1.3411 - acc: 0.4687 - val_loss: -1.2590 - val_acc: 0.4652\n",
            "Epoch 14/50\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -1.3411 - acc: 0.4699 - val_loss: -1.2595 - val_acc: 0.4652\n",
            "Epoch 15/50\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -1.3411 - acc: 0.4718 - val_loss: -1.2589 - val_acc: 0.4708\n",
            "Epoch 16/50\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -1.3406 - acc: 0.4734 - val_loss: -1.2585 - val_acc: 0.4708\n",
            "Epoch 17/50\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -1.3446 - acc: 0.4730 - val_loss: -1.2584 - val_acc: 0.4652\n",
            "Epoch 18/50\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -1.3459 - acc: 0.4734 - val_loss: -1.2585 - val_acc: 0.4763\n",
            "Epoch 19/50\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -1.3462 - acc: 0.4750 - val_loss: -1.2582 - val_acc: 0.4818\n",
            "Epoch 20/50\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -1.3460 - acc: 0.4749 - val_loss: -1.2587 - val_acc: 0.4818\n",
            "Epoch 21/50\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -1.3476 - acc: 0.4750 - val_loss: -1.2589 - val_acc: 0.4763\n",
            "Epoch 22/50\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -1.3473 - acc: 0.4768 - val_loss: -1.2590 - val_acc: 0.4763\n",
            "Epoch 23/50\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -1.3469 - acc: 0.4764 - val_loss: -1.2585 - val_acc: 0.4763\n",
            "Epoch 24/50\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -1.3453 - acc: 0.4763 - val_loss: -1.2597 - val_acc: 0.4763\n",
            "Epoch 25/50\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -1.3455 - acc: 0.4711 - val_loss: -1.2572 - val_acc: 0.4584\n",
            "Epoch 26/50\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -1.3473 - acc: 0.4661 - val_loss: -1.2580 - val_acc: 0.4697\n",
            "Epoch 27/50\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -1.3488 - acc: 0.4686 - val_loss: -1.2586 - val_acc: 0.4641\n",
            "Epoch 28/50\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -1.3486 - acc: 0.4714 - val_loss: -1.2559 - val_acc: 0.4708\n",
            "Epoch 29/50\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -1.3469 - acc: 0.4793 - val_loss: -1.2572 - val_acc: 0.4762\n",
            "Epoch 30/50\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -1.3476 - acc: 0.4838 - val_loss: -1.2564 - val_acc: 0.4862\n",
            "Epoch 31/50\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -1.3478 - acc: 0.4849 - val_loss: -1.2563 - val_acc: 0.4862\n",
            "Epoch 32/50\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -1.3491 - acc: 0.4836 - val_loss: -1.2578 - val_acc: 0.4818\n",
            "Epoch 33/50\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -1.3487 - acc: 0.4827 - val_loss: -1.2575 - val_acc: 0.4763\n",
            "Epoch 34/50\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -1.3489 - acc: 0.4834 - val_loss: -1.2582 - val_acc: 0.4829\n",
            "Epoch 35/50\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -1.3496 - acc: 0.4836 - val_loss: -1.2592 - val_acc: 0.4884\n",
            "Epoch 36/50\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -1.3487 - acc: 0.4843 - val_loss: -1.2586 - val_acc: 0.4829\n",
            "Epoch 37/50\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -1.3494 - acc: 0.4837 - val_loss: -1.2587 - val_acc: 0.4829\n",
            "Epoch 38/50\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -1.3493 - acc: 0.4835 - val_loss: -1.2588 - val_acc: 0.4829\n",
            "Epoch 39/50\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -1.3491 - acc: 0.4828 - val_loss: -1.2605 - val_acc: 0.4829\n",
            "Epoch 40/50\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -1.3493 - acc: 0.4820 - val_loss: -1.2598 - val_acc: 0.4829\n",
            "Epoch 41/50\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -1.3505 - acc: 0.4820 - val_loss: -1.2600 - val_acc: 0.4829\n",
            "Epoch 42/50\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -1.3501 - acc: 0.4823 - val_loss: -1.2598 - val_acc: 0.4829\n",
            "Epoch 43/50\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -1.3494 - acc: 0.4849 - val_loss: -1.2590 - val_acc: 0.4884\n",
            "Epoch 44/50\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -1.3497 - acc: 0.4858 - val_loss: -1.2595 - val_acc: 0.4829\n",
            "Epoch 45/50\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -1.3497 - acc: 0.4852 - val_loss: -1.2594 - val_acc: 0.4884\n",
            "Epoch 46/50\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -1.3494 - acc: 0.4861 - val_loss: -1.2590 - val_acc: 0.4884\n",
            "Epoch 47/50\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -1.3485 - acc: 0.4850 - val_loss: -1.2569 - val_acc: 0.4829\n",
            "Epoch 48/50\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -1.3353 - acc: 0.4881 - val_loss: -1.2597 - val_acc: 0.4928\n",
            "Epoch 49/50\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -1.3488 - acc: 0.4940 - val_loss: -1.2570 - val_acc: 0.4873\n",
            "Epoch 50/50\n",
            "2230/2230 [==============================] - 12s 5ms/step - loss: -1.3263 - acc: 0.4941 - val_loss: -1.2566 - val_acc: 0.4873\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f64f7357e48>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPuIoYQZxmth",
        "colab_type": "code",
        "outputId": "e080e6d0-302a-4c32-9921-7a90492b5729",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "l=model.predict(x_train)\n",
        "vcnp = 0\n",
        "vfnp = 0\n",
        "for i in range(len(l)):\n",
        "  for j in range(len(l[i])):\n",
        "    for k in range (len(indice_Vc[i])):\n",
        "      if indice_Vc[i][k] == j:\n",
        "        if int(l[i][j]) == 2:\n",
        "          vcnp +=1\n",
        "    for p in range (len(indice_Vf[i])):\n",
        "      if indice_Vf[i][p] == j:\n",
        "        if int(l[i][j]) == 1:\n",
        "          vfnp +=1\n",
        "print(vcnp)\n",
        "print(vfnp)"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "1256\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}